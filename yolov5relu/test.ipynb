{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb97769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "from utils.plots import plot_images, output_to_target, plot_study_txt\n",
    "from utils.torch_utils import select_device, time_synchronized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b949008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data,\n",
    "         weights=None,\n",
    "         batch_size=32,\n",
    "         imgsz=640,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,  # for NMS\n",
    "         save_json=False,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         verbose=False,\n",
    "         model=None,\n",
    "         dataloader=None,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         save_txt=False,  # for auto-labelling\n",
    "         save_hybrid=False,  # for hybrid auto-labelling\n",
    "         save_conf=False,  # save auto-label confidences\n",
    "         plots=True,\n",
    "         wandb_logger=None,\n",
    "         compute_loss=None,\n",
    "         half_precision=True,\n",
    "         is_coco=False):\n",
    "    # Initialize/load model and set device\n",
    "    training = model is not None\n",
    "    print('Model %s' % (model if model else 0))\n",
    "    print('training %d'%training)\n",
    "    if training:  # called by train.py\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "    else:  # called directly\n",
    "        set_logging()\n",
    "        device = select_device('', batch_size=batch_size)\n",
    "\n",
    "        # Directories\n",
    "        save_dir = Path(increment_path(Path('runs/test') / 'exp', exist_ok=True))  # increment run\n",
    "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "        # Load model\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "        imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
    "\n",
    "        # Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99\n",
    "        # if device.type != 'cpu' and torch.cuda.device_count() > 1:\n",
    "        #     model = nn.DataParallel(model)\n",
    "\n",
    "    # Half\n",
    "    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    if isinstance(data, str):\n",
    "        is_coco = data.endswith('coco.yaml')\n",
    "        with open(data) as f:\n",
    "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Logging\n",
    "    log_imgs = 0\n",
    "    if wandb_logger and wandb_logger.wandb:\n",
    "        log_imgs = min(wandb_logger.log_imgs, 100)\n",
    "    # Dataloader\n",
    "    if not training:\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "#         task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "        task = 'test'\n",
    "        dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n",
    "                                       prefix=colorstr(f'{task}: '))[0]\n",
    "\n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = time_synchronized()\n",
    "            out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "            t0 += time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if compute_loss:\n",
    "                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
    "            t = time_synchronized()\n",
    "            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
    "            t1 += time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path = Path(paths[si])\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Append to text file\n",
    "            if save_txt:\n",
    "                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                for *xyxy, conf, cls in predn.tolist():\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            # W&B logging - Media Panel Plots\n",
    "            if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n",
    "                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
    "                    box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
    "                                 \"class_id\": int(cls),\n",
    "                                 \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
    "                                 \"scores\": {\"class_score\": conf},\n",
    "                                 \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
    "                    boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n",
    "                    wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
    "            wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "                box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append({'image_id': image_id,\n",
    "                                  'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
    "                                  'bbox': [round(x, 3) for x in b],\n",
    "                                  'score': round(p[4], 5)})\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "        # Plot images\n",
    "        if plots and batch_i < 3:\n",
    "            f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n",
    "            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
    "            f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n",
    "            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
    "    if not training:\n",
    "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "    # Plots\n",
    "    if plots:\n",
    "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "        if wandb_logger and wandb_logger.wandb:\n",
    "            val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n",
    "            wandb_logger.log({\"Validation\": val_batches})\n",
    "    if wandb_images:\n",
    "        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and len(jdict):\n",
    "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
    "        anno_json = '../coco/annotations/instances_val2017.json'  # annotations json\n",
    "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "        print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n",
    "        with open(pred_json, 'w') as f:\n",
    "            json.dump(jdict, f)\n",
    "\n",
    "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            anno = COCO(anno_json)  # init annotations api\n",
    "            pred = anno.loadRes(pred_json)  # init predictions api\n",
    "            eval = COCOeval(anno, pred, 'bbox')\n",
    "            if is_coco:\n",
    "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "            eval.evaluate()\n",
    "            eval.accumulate()\n",
    "            eval.summarize()\n",
    "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except Exception as e:\n",
    "            print(f'pycocotools unable to run: {e}')\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    if not training:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef146b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd25433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e4cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b22d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83208ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/voc.yaml'\n",
    "weights = 'yolov5s.pt'\n",
    "batch_size=32\n",
    "imgsz=640\n",
    "conf_thres=0.001\n",
    "iou_thres=0.6  # for NMS\n",
    "save_json=False\n",
    "single_cls=False\n",
    "augment=False\n",
    "verbose=False\n",
    "model=None\n",
    "dataloader=None\n",
    "save_dir=Path('')  # for saving images\n",
    "save_txt=False # for auto-labelling\n",
    "save_hybrid=False  # for hybrid auto-labelling\n",
    "save_conf=False  # save auto-label confidences\n",
    "plots=True\n",
    "wandb_logger=None\n",
    "compute_loss=None\n",
    "half_precision=True\n",
    "is_coco=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e2283d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2021-4-17 torch 1.7.1+cu101 CUDA:0 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:1 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:2 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:3 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "training 0\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients, 17.0 GFLOPS\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "\u001b[34m\u001b[1mval: \u001b[0mError loading data from ../VOC/images/val/: \u001b[34m\u001b[1mval: \u001b[0mNo images found\nSee https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-4cbffc81b4e1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, img_size, batch_size, augment, hyp, rect, image_weights, cache_images, single_cls, stride, pad, prefix)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;31m# self.img_files = sorted([x for x in f if x.suffix[1:].lower() in img_formats])  # pathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{prefix}No images found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \u001b[34m\u001b[1mval: \u001b[0mNo images found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3dc266dea8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     dataloader = create_dataloader(data[task], imgsz, batch_size, gs, pad=0.5, rect=True,\n\u001b[0;32m---> 52\u001b[0;31m                                    prefix=colorstr(f'{task}: '))[0]\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-4cbffc81b4e1>\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(path, imgsz, batch_size, stride, hyp, augment, cache, pad, rect, rank, world_size, workers, image_weights, quad, prefix)\u001b[0m\n\u001b[1;32m     69\u001b[0m                                       \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                       \u001b[0mimage_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                                       prefix=prefix)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-4cbffc81b4e1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, img_size, batch_size, augment, hyp, rect, image_weights, cache_images, single_cls, stride, pad, prefix)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{prefix}No images found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{prefix}Error loading data from {path}: {e}\\nSee {help_url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Check cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: \u001b[34m\u001b[1mval: \u001b[0mError loading data from ../VOC/images/val/: \u001b[34m\u001b[1mval: \u001b[0mNo images found\nSee https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
     ]
    }
   ],
   "source": [
    "# Initialize/load model and set device\n",
    "training = model is not None\n",
    "print('Model %s' % (model if model else 0))\n",
    "print('training %d'%training)\n",
    "if training:  # called by train.py\n",
    "    device = next(model.parameters()).device  # get model device\n",
    "\n",
    "else:  # called directly\n",
    "    set_logging()\n",
    "    device = select_device('', batch_size=batch_size)\n",
    "\n",
    "    # Directories\n",
    "    save_dir = Path(increment_path(Path('runs/test') / 'exp', exist_ok=True))  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
    "\n",
    "    # Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99\n",
    "    # if device.type != 'cpu' and torch.cuda.device_count() > 1:\n",
    "    #     model = nn.DataParallel(model)\n",
    "\n",
    "# Half\n",
    "half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()\n",
    "\n",
    "# Configure\n",
    "model.eval()\n",
    "if isinstance(data, str):\n",
    "    is_coco = data.endswith('coco.yaml')\n",
    "    with open(data) as f:\n",
    "        data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "check_dataset(data)  # check\n",
    "nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()\n",
    "\n",
    "# Logging\n",
    "log_imgs = 0\n",
    "if wandb_logger and wandb_logger.wandb:\n",
    "    log_imgs = min(wandb_logger.log_imgs, 100)\n",
    "# Dataloader\n",
    "if not training:\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "#         task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "    task = 'val'\n",
    "    dataloader = create_dataloader(data[task], imgsz, batch_size, gs, pad=0.5, rect=True,\n",
    "                                   prefix=colorstr(f'{task}: '))[0]\n",
    "\n",
    "seen = 0\n",
    "confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "coco91class = coco80_to_coco91_class()\n",
    "s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "loss = torch.zeros(3, device=device)\n",
    "jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "    img = img.to(device, non_blocking=True)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    targets = targets.to(device)\n",
    "    nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run model\n",
    "        t = time_synchronized()\n",
    "        out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "        t0 += time_synchronized() - t\n",
    "\n",
    "        # Compute loss\n",
    "        if compute_loss:\n",
    "            loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
    "\n",
    "        # Run NMS\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
    "        t = time_synchronized()\n",
    "        out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
    "        t1 += time_synchronized() - t\n",
    "\n",
    "    # Statistics per image\n",
    "    for si, pred in enumerate(out):\n",
    "        labels = targets[targets[:, 0] == si, 1:]\n",
    "        nl = len(labels)\n",
    "        tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "        path = Path(paths[si])\n",
    "        seen += 1\n",
    "\n",
    "        if len(pred) == 0:\n",
    "            if nl:\n",
    "                stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "            continue\n",
    "\n",
    "        # Predictions\n",
    "        predn = pred.clone()\n",
    "        scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
    "\n",
    "        # Append to text file\n",
    "        if save_txt:\n",
    "            gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            for *xyxy, conf, cls in predn.tolist():\n",
    "                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
    "                    f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "        # W&B logging - Media Panel Plots\n",
    "        if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n",
    "            if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
    "                box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
    "                             \"class_id\": int(cls),\n",
    "                             \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
    "                             \"scores\": {\"class_score\": conf},\n",
    "                             \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
    "                boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n",
    "                wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
    "        wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n",
    "\n",
    "        # Append to pycocotools JSON dictionary\n",
    "        if save_json:\n",
    "            # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "            image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "            box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "            box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "            for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                jdict.append({'image_id': image_id,\n",
    "                              'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
    "                              'bbox': [round(x, 3) for x in b],\n",
    "                              'score': round(p[4], 5)})\n",
    "\n",
    "        # Assign all predictions as incorrect\n",
    "        correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "        if nl:\n",
    "            detected = []  # target indices\n",
    "            tcls_tensor = labels[:, 0]\n",
    "\n",
    "            # target boxes\n",
    "            tbox = xywh2xyxy(labels[:, 1:5])\n",
    "            scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "            if plots:\n",
    "                confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
    "\n",
    "            # Per target class\n",
    "            for cls in torch.unique(tcls_tensor):\n",
    "                ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                # Search for detections\n",
    "                if pi.shape[0]:\n",
    "                    # Prediction to target ious\n",
    "                    ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                    # Append detections\n",
    "                    detected_set = set()\n",
    "                    for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                        d = ti[i[j]]  # detected target\n",
    "                        if d.item() not in detected_set:\n",
    "                            detected_set.add(d.item())\n",
    "                            detected.append(d)\n",
    "                            correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                            if len(detected) == nl:  # all targets already located in image\n",
    "                                break\n",
    "\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "    # Plot images\n",
    "    if plots and batch_i < 3:\n",
    "        f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n",
    "        Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
    "        f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n",
    "        Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
    "\n",
    "# Compute statistics\n",
    "stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "    nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "else:\n",
    "    nt = torch.zeros(1)\n",
    "\n",
    "# Print results\n",
    "pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "# Print results per class\n",
    "if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "    for i, c in enumerate(ap_class):\n",
    "        print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "# Print speeds\n",
    "t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
    "if not training:\n",
    "    print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "# Plots\n",
    "if plots:\n",
    "    confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "    if wandb_logger and wandb_logger.wandb:\n",
    "        val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n",
    "        wandb_logger.log({\"Validation\": val_batches})\n",
    "if wandb_images:\n",
    "    wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
    "\n",
    "# Save JSON\n",
    "if save_json and len(jdict):\n",
    "    w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
    "    anno_json = '../coco/annotations/instances_val2017.json'  # annotations json\n",
    "    pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "    print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n",
    "    with open(pred_json, 'w') as f:\n",
    "        json.dump(jdict, f)\n",
    "\n",
    "    try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "        from pycocotools.coco import COCO\n",
    "        from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "        anno = COCO(anno_json)  # init annotations api\n",
    "        pred = anno.loadRes(pred_json)  # init predictions api\n",
    "        eval = COCOeval(anno, pred, 'bbox')\n",
    "        if is_coco:\n",
    "            eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "        eval.evaluate()\n",
    "        eval.accumulate()\n",
    "        eval.summarize()\n",
    "        map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "    except Exception as e:\n",
    "        print(f'pycocotools unable to run: {e}')\n",
    "\n",
    "# Return results\n",
    "model.float()  # for training\n",
    "if not training:\n",
    "    s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "    print(f\"Results saved to {save_dir}{s}\")\n",
    "maps = np.zeros(nc) + map\n",
    "for i, c in enumerate(ap_class):\n",
    "    maps[c] = ap[i]\n",
    "#     return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0711226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtrainval_11-May-2012.zip ...\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtest_06-Nov-2007.zip ...\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VOCtrainval_06-Nov-2007.zip ...\n",
      "      %%%  T TToootttaaalll            %%%  R eRcReeievcceeeiidvve ed%d   %X% f eXXrfdf ee rrAdd v  e ArAvavegeer raaggSee p SeSeppdeee e dd   T  i  mTeTii m me e    T  i  mTe i mT ei  m  e    T  Tii mm ee   T  iCCmuuerr r reCennuttr\n",
      "\n",
      "r  e  n  t  \n",
      "                                                                                     D Dl lo oa a dd   U Dp llUoopaalddo  a  d U pT lo otTaoadtl  a  l  S Tp oen ttSap le   n   tLSe pf et n  t LS pe ef et d \n",
      "pt  e  e0dS \n",
      "e  d  \n",
      "100   635  100   635    0     0     93      0  0:00:06  0:00:06 --:--:--   148 0 0               0 00    -  -   :  - - 00:  - -- -  :- - --0:: ------: :----- --::---:---: ----:- -:--- -- ::- - - -:0 ---- : - - : -0-     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:02:00 --:--:--     0          00  ----::----::----    00::0011::5599  ----::----::----          00:01:31 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:02:00 --:--:--     0\n",
      "curl: (35) gnutls_handshake() failed: The TLS connection was non-properly terminated.\n",
      "curl: (35) gnutls_handshake() failed: The TLS connection was non-properly terminated.\n",
      "100 1859M  100 1859M    0     0  4791k      0  0:06:37  0:06:37 --:--:-- 5147k  818M    0     0  3593k      0  0:08:49  0:03:53  0:04:56 7843kM   76 1416M    0     0  4529k      0  0:07:00  0:05:20  0:01:40 9548k\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!sh data/scripts/get_voc.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ce1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aef29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a649c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/voc.yaml'\n",
    "weights = 'yolov5s.pt'\n",
    "batch_size = 32\n",
    "img_size = 640\n",
    "conf_thres = 0.1\n",
    "iou_thres = 0.6\n",
    "save_json = True\n",
    "single_cls = False\n",
    "augment = False\n",
    "verbose = False\n",
    "save_txt=False,  # for auto-labelling\n",
    "save_hybrid=False,  # for hybrid auto-labelling\n",
    "save_conf=False,  # save auto-label confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee4f143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2021-4-17 torch 1.7.1+cu101 CUDA:0 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:1 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:2 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "                                     CUDA:3 (GeForce RTX 2080 Ti, 10989.4375MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "training 0\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients, 17.0 GFLOPS\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-262339912353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test(data=data,weights=weights,batch_size=batch_size,imgsz=img_size,conf_thres=conf_thres,\n\u001b[1;32m      2\u001b[0m      \u001b[0miou_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou_thres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m      verbose=verbose,save_txt=save_txt,save_hybrid=save_hybrid,save_conf=save_conf)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-93d7df42a1a8>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data, weights, batch_size, imgsz, conf_thres, iou_thres, save_json, single_cls, augment, verbose, model, dataloader, save_dir, save_txt, save_hybrid, save_conf, plots, wandb_logger, compute_loss, half_precision, is_coco)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#         task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n\u001b[0m\u001b[1;32m     73\u001b[0m                                        prefix=colorstr(f'{task}: '))[0]\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "test(data=data,weights=weights,batch_size=batch_size,imgsz=img_size,conf_thres=conf_thres,\n",
    "     iou_thres=iou_thres,save_json=save_json,single_cls=single_cls,augment=augment,\n",
    "     verbose=verbose,save_txt=save_txt,save_hybrid=save_hybrid,save_conf=save_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22b21106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset utils and dataloaders\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from itertools import repeat\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ExifTags\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.general import check_requirements, xyxy2xywh, xywh2xyxy, xywhn2xyxy, xyn2xy, segment2box, segments2boxes, \\\n",
    "    resample_segments, clean_str\n",
    "from utils.torch_utils import torch_distributed_zero_first\n",
    "\n",
    "# Parameters\n",
    "help_url = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n",
    "img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n",
    "vid_formats = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get orientation exif tag\n",
    "for orientation in ExifTags.TAGS.keys():\n",
    "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
    "        break\n",
    "\n",
    "\n",
    "def get_hash(files):\n",
    "    # Returns a single hash value of a list of files\n",
    "    return sum(os.path.getsize(f) for f in files if os.path.isfile(f))\n",
    "\n",
    "\n",
    "def exif_size(img):\n",
    "    # Returns exif-corrected PIL size\n",
    "    s = img.size  # (width, height)\n",
    "    try:\n",
    "        rotation = dict(img._getexif().items())[orientation]\n",
    "        if rotation == 6:  # rotation 270\n",
    "            s = (s[1], s[0])\n",
    "        elif rotation == 8:  # rotation 90\n",
    "            s = (s[1], s[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def create_dataloader(path, imgsz, batch_size, stride, hyp=None, augment=False, cache=False, pad=0.0, rect=False,\n",
    "                      rank=-1, world_size=1, workers=8, image_weights=False, quad=False, prefix=''):\n",
    "    # Make sure only the first process in DDP process the dataset first, and the following others can use the cache\n",
    "    with torch_distributed_zero_first(rank):\n",
    "        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
    "                                      augment=augment,  # augment images\n",
    "                                      hyp=hyp,  # augmentation hyperparameters\n",
    "                                      rect=rect,  # rectangular training\n",
    "                                      cache_images=cache,\n",
    "                                      single_cls=False,\n",
    "                                      stride=int(stride),\n",
    "                                      pad=pad,\n",
    "                                      image_weights=image_weights,\n",
    "                                      prefix=prefix)\n",
    "\n",
    "    batch_size = min(batch_size, len(dataset))\n",
    "    nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None\n",
    "    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader\n",
    "    # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader()\n",
    "    dataloader = loader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=nw,\n",
    "                        sampler=sampler,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn)\n",
    "    return dataloader, dataset\n",
    "\n",
    "\n",
    "class InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):\n",
    "    \"\"\" Dataloader that reuses workers\n",
    "\n",
    "    Uses same syntax as vanilla DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)\n",
    "\n",
    "\n",
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever\n",
    "\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)\n",
    "\n",
    "\n",
    "class LoadImages:  # for inference\n",
    "    def __init__(self, path, img_size=640, stride=32):\n",
    "        p = str(Path(path).absolute())  # os-agnostic absolute path\n",
    "        if '*' in p:\n",
    "            files = sorted(glob.glob(p, recursive=True))  # glob\n",
    "        elif os.path.isdir(p):\n",
    "            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n",
    "        elif os.path.isfile(p):\n",
    "            files = [p]  # files\n",
    "        else:\n",
    "            raise Exception(f'ERROR: {p} does not exist')\n",
    "\n",
    "        images = [x for x in files if x.split('.')[-1].lower() in img_formats]\n",
    "        videos = [x for x in files if x.split('.')[-1].lower() in vid_formats]\n",
    "        ni, nv = len(images), len(videos)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.files = images + videos\n",
    "        self.nf = ni + nv  # number of files\n",
    "        self.video_flag = [False] * ni + [True] * nv\n",
    "        self.mode = 'image'\n",
    "        if any(videos):\n",
    "            self.new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
    "                            f'Supported formats are:\\nimages: {img_formats}\\nvideos: {vid_formats}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.nf:\n",
    "            raise StopIteration\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        if self.video_flag[self.count]:\n",
    "            # Read video\n",
    "            self.mode = 'video'\n",
    "            ret_val, img0 = self.cap.read()\n",
    "            if not ret_val:\n",
    "                self.count += 1\n",
    "                self.cap.release()\n",
    "                if self.count == self.nf:  # last video\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    path = self.files[self.count]\n",
    "                    self.new_video(path)\n",
    "                    ret_val, img0 = self.cap.read()\n",
    "\n",
    "            self.frame += 1\n",
    "            print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.nframes}) {path}: ', end='')\n",
    "\n",
    "        else:\n",
    "            # Read image\n",
    "            self.count += 1\n",
    "            img0 = cv2.imread(path)  # BGR\n",
    "            assert img0 is not None, 'Image Not Found ' + path\n",
    "            print(f'image {self.count}/{self.nf} {path}: ', end='')\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0, self.cap\n",
    "\n",
    "    def new_video(self, path):\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.nframes = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nf  # number of files\n",
    "\n",
    "\n",
    "class LoadWebcam:  # for inference\n",
    "    def __init__(self, pipe='0', img_size=640, stride=32):\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "\n",
    "        if pipe.isnumeric():\n",
    "            pipe = eval(pipe)  # local camera\n",
    "        # pipe = 'rtsp://192.168.1.64/1'  # IP camera\n",
    "        # pipe = 'rtsp://username:password@192.168.1.64/1'  # IP camera with login\n",
    "        # pipe = 'http://wmccpinetop.axiscam.net/mjpg/video.mjpg'  # IP golf camera\n",
    "\n",
    "        self.pipe = pipe\n",
    "        self.cap = cv2.VideoCapture(pipe)  # video capture object\n",
    "        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "            self.cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            raise StopIteration\n",
    "\n",
    "        # Read frame\n",
    "        if self.pipe == 0:  # local camera\n",
    "            ret_val, img0 = self.cap.read()\n",
    "            img0 = cv2.flip(img0, 1)  # flip left-right\n",
    "        else:  # IP camera\n",
    "            n = 0\n",
    "            while True:\n",
    "                n += 1\n",
    "                self.cap.grab()\n",
    "                if n % 30 == 0:  # skip frames\n",
    "                    ret_val, img0 = self.cap.retrieve()\n",
    "                    if ret_val:\n",
    "                        break\n",
    "\n",
    "        # Print\n",
    "        assert ret_val, f'Camera Error {self.pipe}'\n",
    "        img_path = 'webcam.jpg'\n",
    "        print(f'webcam {self.count}: ', end='')\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return img_path, img, img0, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class LoadStreams:  # multiple IP or RTSP cameras\n",
    "    def __init__(self, sources='streams.txt', img_size=640, stride=32):\n",
    "        self.mode = 'stream'\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "\n",
    "        if os.path.isfile(sources):\n",
    "            with open(sources, 'r') as f:\n",
    "                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]\n",
    "        else:\n",
    "            sources = [sources]\n",
    "\n",
    "        n = len(sources)\n",
    "        self.imgs = [None] * n\n",
    "        self.sources = [clean_str(x) for x in sources]  # clean source names for later\n",
    "        for i, s in enumerate(sources):  # index, source\n",
    "            # Start thread to read frames from video stream\n",
    "            print(f'{i + 1}/{n}: {s}... ', end='')\n",
    "            if 'youtube.com/' in s or 'youtu.be/' in s:  # if source is YouTube video\n",
    "                check_requirements(('pafy', 'youtube_dl'))\n",
    "                import pafy\n",
    "                s = pafy.new(s).getbest(preftype=\"mp4\").url  # YouTube URL\n",
    "            s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\n",
    "            cap = cv2.VideoCapture(s)\n",
    "            assert cap.isOpened(), f'Failed to open {s}'\n",
    "            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            self.fps = cap.get(cv2.CAP_PROP_FPS) % 100\n",
    "\n",
    "            _, self.imgs[i] = cap.read()  # guarantee first frame\n",
    "            thread = Thread(target=self.update, args=([i, cap]), daemon=True)\n",
    "            print(f' success ({w}x{h} at {self.fps:.2f} FPS).')\n",
    "            thread.start()\n",
    "        print('')  # newline\n",
    "\n",
    "        # check for common shapes\n",
    "        s = np.stack([letterbox(x, self.img_size, stride=self.stride)[0].shape for x in self.imgs], 0)  # shapes\n",
    "        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n",
    "        if not self.rect:\n",
    "            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')\n",
    "\n",
    "    def update(self, index, cap):\n",
    "        # Read next stream frame in a daemon thread\n",
    "        n = 0\n",
    "        while cap.isOpened():\n",
    "            n += 1\n",
    "            # _, self.imgs[index] = cap.read()\n",
    "            cap.grab()\n",
    "            if n == 4:  # read every 4th frame\n",
    "                success, im = cap.retrieve()\n",
    "                self.imgs[index] = im if success else self.imgs[index] * 0\n",
    "                n = 0\n",
    "            time.sleep(1 / self.fps)  # wait time\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        img0 = self.imgs.copy()\n",
    "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "            cv2.destroyAllWindows()\n",
    "            raise StopIteration\n",
    "\n",
    "        # Letterbox\n",
    "        img = [letterbox(x, self.img_size, auto=self.rect, stride=self.stride)[0] for x in img0]\n",
    "\n",
    "        # Stack\n",
    "        img = np.stack(img, 0)\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)  # BGR to RGB, to bsx3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return self.sources, img, img0, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return 0  # 1E12 frames = 32 streams at 30 FPS for 30 years\n",
    "\n",
    "\n",
    "def img2label_paths(img_paths):\n",
    "    # Define label paths as a function of image paths\n",
    "    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings\n",
    "    return ['txt'.join(x.replace(sa, sb, 1).rsplit(x.split('.')[-1], 1)) for x in img_paths]\n",
    "\n",
    "\n",
    "class LoadImagesAndLabels(Dataset):  # for training/testing\n",
    "    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n",
    "                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.hyp = hyp\n",
    "        self.image_weights = image_weights\n",
    "        self.rect = False if image_weights else rect\n",
    "        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n",
    "        self.mosaic_border = [-img_size // 2, -img_size // 2]\n",
    "        self.stride = stride\n",
    "        self.path = path\n",
    "\n",
    "        try:\n",
    "            f = []  # image files\n",
    "            for p in path if isinstance(path, list) else [path]:\n",
    "                p = Path(p)  # os-agnostic\n",
    "                if p.is_dir():  # dir\n",
    "                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
    "                    # f = list(p.rglob('**/*.*'))  # pathlib\n",
    "                elif p.is_file():  # file\n",
    "                    with open(p, 'r') as t:\n",
    "                        t = t.read().strip().splitlines()\n",
    "                        parent = str(p.parent) + os.sep\n",
    "                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n",
    "                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
    "                else:\n",
    "                    raise Exception(f'{prefix}{p} does not exist')\n",
    "            self.img_files = sorted([x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in img_formats])\n",
    "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in img_formats])  # pathlib\n",
    "            assert self.img_files, f'{prefix}No images found'\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {help_url}')\n",
    "\n",
    "        # Check cache\n",
    "        self.label_files = img2label_paths(self.img_files)  # labels\n",
    "        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')  # cached labels\n",
    "        if cache_path.is_file():\n",
    "            cache, exists = torch.load(cache_path), True  # load\n",
    "            if cache['hash'] != get_hash(self.label_files + self.img_files) or 'version' not in cache:  # changed\n",
    "                cache, exists = self.cache_labels(cache_path, prefix), False  # re-cache\n",
    "        else:\n",
    "            cache, exists = self.cache_labels(cache_path, prefix), False  # cache\n",
    "\n",
    "        # Display cache\n",
    "        nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupted, total\n",
    "        if exists:\n",
    "            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
    "            tqdm(None, desc=prefix + d, total=n, initial=n)  # display cache results\n",
    "        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {help_url}'\n",
    "\n",
    "        # Read cache\n",
    "        cache.pop('hash')  # remove hash\n",
    "        cache.pop('version')  # remove version\n",
    "        labels, shapes, self.segments = zip(*cache.values())\n",
    "        self.labels = list(labels)\n",
    "        self.shapes = np.array(shapes, dtype=np.float64)\n",
    "        self.img_files = list(cache.keys())  # update\n",
    "        self.label_files = img2label_paths(cache.keys())  # update\n",
    "        if single_cls:\n",
    "            for x in self.labels:\n",
    "                x[:, 0] = 0\n",
    "\n",
    "        n = len(shapes)  # number of images\n",
    "        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n",
    "        nb = bi[-1] + 1  # number of batches\n",
    "        self.batch = bi  # batch index of image\n",
    "        self.n = n\n",
    "        self.indices = range(n)\n",
    "\n",
    "        # Rectangular Training\n",
    "        if self.rect:\n",
    "            # Sort by aspect ratio\n",
    "            s = self.shapes  # wh\n",
    "            ar = s[:, 1] / s[:, 0]  # aspect ratio\n",
    "            irect = ar.argsort()\n",
    "            self.img_files = [self.img_files[i] for i in irect]\n",
    "            self.label_files = [self.label_files[i] for i in irect]\n",
    "            self.labels = [self.labels[i] for i in irect]\n",
    "            self.shapes = s[irect]  # wh\n",
    "            ar = ar[irect]\n",
    "\n",
    "            # Set training image shapes\n",
    "            shapes = [[1, 1]] * nb\n",
    "            for i in range(nb):\n",
    "                ari = ar[bi == i]\n",
    "                mini, maxi = ari.min(), ari.max()\n",
    "                if maxi < 1:\n",
    "                    shapes[i] = [maxi, 1]\n",
    "                elif mini > 1:\n",
    "                    shapes[i] = [1, 1 / mini]\n",
    "\n",
    "            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n",
    "\n",
    "        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n",
    "        self.imgs = [None] * n\n",
    "        if cache_images:\n",
    "            gb = 0  # Gigabytes of cached images\n",
    "            self.img_hw0, self.img_hw = [None] * n, [None] * n\n",
    "            results = ThreadPool(8).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))  # 8 threads\n",
    "            pbar = tqdm(enumerate(results), total=n)\n",
    "            for i, x in pbar:\n",
    "                self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # img, hw_original, hw_resized = load_image(self, i)\n",
    "                gb += self.imgs[i].nbytes\n",
    "                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB)'\n",
    "            pbar.close()\n",
    "\n",
    "    def cache_labels(self, path=Path('./labels.cache'), prefix=''):\n",
    "        # Cache dataset labels, check images and read shapes\n",
    "        x = {}  # dict\n",
    "        nm, nf, ne, nc = 0, 0, 0, 0  # number missing, found, empty, duplicate\n",
    "        pbar = tqdm(zip(self.img_files, self.label_files), desc='Scanning images', total=len(self.img_files))\n",
    "        for i, (im_file, lb_file) in enumerate(pbar):\n",
    "            try:\n",
    "                # verify images\n",
    "                im = Image.open(im_file)\n",
    "                im.verify()  # PIL verify\n",
    "                shape = exif_size(im)  # image size\n",
    "                segments = []  # instance segments\n",
    "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
    "                assert im.format.lower() in img_formats, f'invalid image format {im.format}'\n",
    "\n",
    "                # verify labels\n",
    "                if os.path.isfile(lb_file):\n",
    "                    nf += 1  # label found\n",
    "                    with open(lb_file, 'r') as f:\n",
    "                        l = [x.split() for x in f.read().strip().splitlines()]\n",
    "                        if any([len(x) > 8 for x in l]):  # is segment\n",
    "                            classes = np.array([x[0] for x in l], dtype=np.float32)\n",
    "                            segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (cls, xy1...)\n",
    "                            l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\n",
    "                        l = np.array(l, dtype=np.float32)\n",
    "                    if len(l):\n",
    "                        assert l.shape[1] == 5, 'labels require 5 columns each'\n",
    "                        assert (l >= 0).all(), 'negative labels'\n",
    "                        assert (l[:, 1:] <= 1).all(), 'non-normalized or out of bounds coordinate labels'\n",
    "                        assert np.unique(l, axis=0).shape[0] == l.shape[0], 'duplicate labels'\n",
    "                    else:\n",
    "                        ne += 1  # label empty\n",
    "                        l = np.zeros((0, 5), dtype=np.float32)\n",
    "                else:\n",
    "                    nm += 1  # label missing\n",
    "                    l = np.zeros((0, 5), dtype=np.float32)\n",
    "                x[im_file] = [l, shape, segments]\n",
    "            except Exception as e:\n",
    "                nc += 1\n",
    "                print(f'{prefix}WARNING: Ignoring corrupted image and/or label {im_file}: {e}')\n",
    "\n",
    "            pbar.desc = f\"{prefix}Scanning '{path.parent / path.stem}' images and labels... \" \\\n",
    "                        f\"{nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
    "        pbar.close()\n",
    "\n",
    "        if nf == 0:\n",
    "            print(f'{prefix}WARNING: No labels found in {path}. See {help_url}')\n",
    "\n",
    "        x['hash'] = get_hash(self.label_files + self.img_files)\n",
    "        x['results'] = nf, nm, ne, nc, i + 1\n",
    "        x['version'] = 0.1  # cache version\n",
    "        torch.save(x, path)  # save for next time\n",
    "        logging.info(f'{prefix}New cache created: {path}')\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    # def __iter__(self):\n",
    "    #     self.count = -1\n",
    "    #     print('ran dataset iter')\n",
    "    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n",
    "    #     return self\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]  # linear, shuffled, or image_weights\n",
    "\n",
    "        hyp = self.hyp\n",
    "        mosaic = self.mosaic and random.random() < hyp['mosaic']\n",
    "        if mosaic:\n",
    "            # Load mosaic\n",
    "            img, labels = load_mosaic(self, index)\n",
    "            shapes = None\n",
    "\n",
    "            # MixUp https://arxiv.org/pdf/1710.09412.pdf\n",
    "            if random.random() < hyp['mixup']:\n",
    "                img2, labels2 = load_mosaic(self, random.randint(0, self.n - 1))\n",
    "                r = np.random.beta(8.0, 8.0)  # mixup ratio, alpha=beta=8.0\n",
    "                img = (img * r + img2 * (1 - r)).astype(np.uint8)\n",
    "                labels = np.concatenate((labels, labels2), 0)\n",
    "\n",
    "        else:\n",
    "            # Load image\n",
    "            img, (h0, w0), (h, w) = load_image(self, index)\n",
    "\n",
    "            # Letterbox\n",
    "            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n",
    "            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n",
    "            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
    "\n",
    "            labels = self.labels[index].copy()\n",
    "            if labels.size:  # normalized xywh to pixel xyxy format\n",
    "                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\n",
    "\n",
    "        if self.augment:\n",
    "            # Augment imagespace\n",
    "            if not mosaic:\n",
    "                img, labels = random_perspective(img, labels,\n",
    "                                                 degrees=hyp['degrees'],\n",
    "                                                 translate=hyp['translate'],\n",
    "                                                 scale=hyp['scale'],\n",
    "                                                 shear=hyp['shear'],\n",
    "                                                 perspective=hyp['perspective'])\n",
    "\n",
    "            # Augment colorspace\n",
    "            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n",
    "\n",
    "            # Apply cutouts\n",
    "            # if random.random() < 0.9:\n",
    "            #     labels = cutout(img, labels)\n",
    "\n",
    "        nL = len(labels)  # number of labels\n",
    "        if nL:\n",
    "            labels[:, 1:5] = xyxy2xywh(labels[:, 1:5])  # convert xyxy to xywh\n",
    "            labels[:, [2, 4]] /= img.shape[0]  # normalized height 0-1\n",
    "            labels[:, [1, 3]] /= img.shape[1]  # normalized width 0-1\n",
    "\n",
    "        if self.augment:\n",
    "            # flip up-down\n",
    "            if random.random() < hyp['flipud']:\n",
    "                img = np.flipud(img)\n",
    "                if nL:\n",
    "                    labels[:, 2] = 1 - labels[:, 2]\n",
    "\n",
    "            # flip left-right\n",
    "            if random.random() < hyp['fliplr']:\n",
    "                img = np.fliplr(img)\n",
    "                if nL:\n",
    "                    labels[:, 1] = 1 - labels[:, 1]\n",
    "\n",
    "        labels_out = torch.zeros((nL, 6))\n",
    "        if nL:\n",
    "            labels_out[:, 1:] = torch.from_numpy(labels)\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        img, label, path, shapes = zip(*batch)  # transposed\n",
    "        for i, l in enumerate(label):\n",
    "            l[:, 0] = i  # add target image index for build_targets()\n",
    "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn4(batch):\n",
    "        img, label, path, shapes = zip(*batch)  # transposed\n",
    "        n = len(shapes) // 4\n",
    "        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\n",
    "\n",
    "        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])\n",
    "        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])\n",
    "        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale\n",
    "        for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\n",
    "            i *= 4\n",
    "            if random.random() < 0.5:\n",
    "                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[\n",
    "                    0].type(img[i].type())\n",
    "                l = label[i]\n",
    "            else:\n",
    "                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)\n",
    "                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\n",
    "            img4.append(im)\n",
    "            label4.append(l)\n",
    "\n",
    "        for i, l in enumerate(label4):\n",
    "            l[:, 0] = i  # add target image index for build_targets()\n",
    "\n",
    "        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4\n",
    "\n",
    "\n",
    "# Ancillary functions --------------------------------------------------------------------------------------------------\n",
    "def load_image(self, index):\n",
    "    # loads 1 image from dataset, returns img, original hw, resized hw\n",
    "    img = self.imgs[index]\n",
    "    if img is None:  # not cached\n",
    "        path = self.img_files[index]\n",
    "        img = cv2.imread(path)  # BGR\n",
    "        assert img is not None, 'Image Not Found ' + path\n",
    "        h0, w0 = img.shape[:2]  # orig hw\n",
    "        r = self.img_size / max(h0, w0)  # resize image to img_size\n",
    "        if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "            interp = cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR\n",
    "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n",
    "    else:\n",
    "        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n",
    "\n",
    "\n",
    "def augment_hsv(img, hgain=0.5, sgain=0.5, vgain=0.5):\n",
    "    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
    "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
    "    dtype = img.dtype  # uint8\n",
    "\n",
    "    x = np.arange(0, 256, dtype=np.int16)\n",
    "    lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n",
    "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\n",
    "\n",
    "\n",
    "def hist_equalize(img, clahe=True, bgr=False):\n",
    "    # Equalize histogram on BGR image 'img' with img.shape(n,m,3) and range 0-255\n",
    "    yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\n",
    "    if clahe:\n",
    "        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\n",
    "    else:\n",
    "        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\n",
    "\n",
    "\n",
    "def load_mosaic(self, index):\n",
    "    # loads images in a 4-mosaic\n",
    "\n",
    "    labels4, segments4 = [], []\n",
    "    s = self.img_size\n",
    "    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]  # mosaic center x, y\n",
    "    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices\n",
    "    for i, index in enumerate(indices):\n",
    "        # Load image\n",
    "        img, _, (h, w) = load_image(self, index)\n",
    "\n",
    "        # place img in img4\n",
    "        if i == 0:  # top left\n",
    "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "        elif i == 1:  # top right\n",
    "            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "        elif i == 2:  # bottom left\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
    "        elif i == 3:  # bottom right\n",
    "            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "\n",
    "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
    "        padw = x1a - x1b\n",
    "        padh = y1a - y1b\n",
    "\n",
    "        # Labels\n",
    "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
    "        if labels.size:\n",
    "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format\n",
    "            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n",
    "        labels4.append(labels)\n",
    "        segments4.extend(segments)\n",
    "\n",
    "    # Concat/clip labels\n",
    "    labels4 = np.concatenate(labels4, 0)\n",
    "    for x in (labels4[:, 1:], *segments4):\n",
    "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
    "    # img4, labels4 = replicate(img4, labels4)  # replicate\n",
    "\n",
    "    # Augment\n",
    "    img4, labels4 = random_perspective(img4, labels4, segments4,\n",
    "                                       degrees=self.hyp['degrees'],\n",
    "                                       translate=self.hyp['translate'],\n",
    "                                       scale=self.hyp['scale'],\n",
    "                                       shear=self.hyp['shear'],\n",
    "                                       perspective=self.hyp['perspective'],\n",
    "                                       border=self.mosaic_border)  # border to remove\n",
    "\n",
    "    return img4, labels4\n",
    "\n",
    "\n",
    "def load_mosaic9(self, index):\n",
    "    # loads images in a 9-mosaic\n",
    "\n",
    "    labels9, segments9 = [], []\n",
    "    s = self.img_size\n",
    "    indices = [index] + random.choices(self.indices, k=8)  # 8 additional image indices\n",
    "    for i, index in enumerate(indices):\n",
    "        # Load image\n",
    "        img, _, (h, w) = load_image(self, index)\n",
    "\n",
    "        # place img in img9\n",
    "        if i == 0:  # center\n",
    "            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
    "            h0, w0 = h, w\n",
    "            c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\n",
    "        elif i == 1:  # top\n",
    "            c = s, s - h, s + w, s\n",
    "        elif i == 2:  # top right\n",
    "            c = s + wp, s - h, s + wp + w, s\n",
    "        elif i == 3:  # right\n",
    "            c = s + w0, s, s + w0 + w, s + h\n",
    "        elif i == 4:  # bottom right\n",
    "            c = s + w0, s + hp, s + w0 + w, s + hp + h\n",
    "        elif i == 5:  # bottom\n",
    "            c = s + w0 - w, s + h0, s + w0, s + h0 + h\n",
    "        elif i == 6:  # bottom left\n",
    "            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\n",
    "        elif i == 7:  # left\n",
    "            c = s - w, s + h0 - h, s, s + h0\n",
    "        elif i == 8:  # top left\n",
    "            c = s - w, s + h0 - hp - h, s, s + h0 - hp\n",
    "\n",
    "        padx, pady = c[:2]\n",
    "        x1, y1, x2, y2 = [max(x, 0) for x in c]  # allocate coords\n",
    "\n",
    "        # Labels\n",
    "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
    "        if labels.size:\n",
    "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # normalized xywh to pixel xyxy format\n",
    "            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\n",
    "        labels9.append(labels)\n",
    "        segments9.extend(segments)\n",
    "\n",
    "        # Image\n",
    "        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\n",
    "        hp, wp = h, w  # height, width previous\n",
    "\n",
    "    # Offset\n",
    "    yc, xc = [int(random.uniform(0, s)) for _ in self.mosaic_border]  # mosaic center x, y\n",
    "    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]\n",
    "\n",
    "    # Concat/clip labels\n",
    "    labels9 = np.concatenate(labels9, 0)\n",
    "    labels9[:, [1, 3]] -= xc\n",
    "    labels9[:, [2, 4]] -= yc\n",
    "    c = np.array([xc, yc])  # centers\n",
    "    segments9 = [x - c for x in segments9]\n",
    "\n",
    "    for x in (labels9[:, 1:], *segments9):\n",
    "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
    "    # img9, labels9 = replicate(img9, labels9)  # replicate\n",
    "\n",
    "    # Augment\n",
    "    img9, labels9 = random_perspective(img9, labels9, segments9,\n",
    "                                       degrees=self.hyp['degrees'],\n",
    "                                       translate=self.hyp['translate'],\n",
    "                                       scale=self.hyp['scale'],\n",
    "                                       shear=self.hyp['shear'],\n",
    "                                       perspective=self.hyp['perspective'],\n",
    "                                       border=self.mosaic_border)  # border to remove\n",
    "\n",
    "    return img9, labels9\n",
    "\n",
    "\n",
    "def replicate(img, labels):\n",
    "    # Replicate labels\n",
    "    h, w = img.shape[:2]\n",
    "    boxes = labels[:, 1:].astype(int)\n",
    "    x1, y1, x2, y2 = boxes.T\n",
    "    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\n",
    "    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\n",
    "        x1b, y1b, x2b, y2b = boxes[i]\n",
    "        bh, bw = y2b - y1b, x2b - x1b\n",
    "        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y\n",
    "        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]\n",
    "        img[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
    "        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)\n",
    "\n",
    "    return img, labels\n",
    "\n",
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def random_perspective(img, targets=(), segments=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0,\n",
    "                       border=(0, 0)):\n",
    "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n",
    "    # targets = [cls, xyxy]\n",
    "\n",
    "    height = img.shape[0] + border[0] * 2  # shape(h,w,c)\n",
    "    width = img.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    C = np.eye(3)\n",
    "    C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n",
    "    C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    P = np.eye(3)\n",
    "    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n",
    "    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    R = np.eye(3)\n",
    "    a = random.uniform(-degrees, degrees)\n",
    "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
    "    s = random.uniform(1 - scale, 1 + scale)\n",
    "    # s = 2 ** random.uniform(-scale, scale)\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    S = np.eye(3)\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
    "\n",
    "    # Translation\n",
    "    T = np.eye(3)\n",
    "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n",
    "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n",
    "\n",
    "    # Combined rotation matrix\n",
    "    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n",
    "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n",
    "        if perspective:\n",
    "            img = cv2.warpPerspective(img, M, dsize=(width, height), borderValue=(114, 114, 114))\n",
    "        else:  # affine\n",
    "            img = cv2.warpAffine(img, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n",
    "\n",
    "    # Visualize\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()\n",
    "    # ax[0].imshow(img[:, :, ::-1])  # base\n",
    "    # ax[1].imshow(img2[:, :, ::-1])  # warped\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        use_segments = any(x.any() for x in segments)\n",
    "        new = np.zeros((n, 4))\n",
    "        if use_segments:  # warp segments\n",
    "            segments = resample_segments(segments)  # upsample\n",
    "            for i, segment in enumerate(segments):\n",
    "                xy = np.ones((len(segment), 3))\n",
    "                xy[:, :2] = segment\n",
    "                xy = xy @ M.T  # transform\n",
    "                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\n",
    "\n",
    "                # clip\n",
    "                new[i] = segment2box(xy, width, height)\n",
    "\n",
    "        else:  # warp boxes\n",
    "            xy = np.ones((n * 4, 3))\n",
    "            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "            xy = xy @ M.T  # transform\n",
    "            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "            # create new boxes\n",
    "            x = xy[:, [0, 2, 4, 6]]\n",
    "            y = xy[:, [1, 3, 5, 7]]\n",
    "            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "            # clip\n",
    "            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\n",
    "            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\n",
    "\n",
    "        # filter candidates\n",
    "        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = new[i]\n",
    "\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "def box_candidates(box1, box2, wh_thr=2, ar_thr=20, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n",
    "    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n",
    "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\n",
    "\n",
    "\n",
    "def cutout(image, labels):\n",
    "    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    def bbox_ioa(box1, box2):\n",
    "        # Returns the intersection over box2 area given box1, box2. box1 is 4, box2 is nx4. boxes are x1y1x2y2\n",
    "        box2 = box2.transpose()\n",
    "\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "\n",
    "        # Intersection area\n",
    "        inter_area = (np.minimum(b1_x2, b2_x2) - np.maximum(b1_x1, b2_x1)).clip(0) * \\\n",
    "                     (np.minimum(b1_y2, b2_y2) - np.maximum(b1_y1, b2_y1)).clip(0)\n",
    "\n",
    "        # box2 area\n",
    "        box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1) + 1e-16\n",
    "\n",
    "        # Intersection over box2 area\n",
    "        return inter_area / box2_area\n",
    "\n",
    "    # create random masks\n",
    "    scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n",
    "    for s in scales:\n",
    "        mask_h = random.randint(1, int(h * s))\n",
    "        mask_w = random.randint(1, int(w * s))\n",
    "\n",
    "        # box\n",
    "        xmin = max(0, random.randint(0, w) - mask_w // 2)\n",
    "        ymin = max(0, random.randint(0, h) - mask_h // 2)\n",
    "        xmax = min(w, xmin + mask_w)\n",
    "        ymax = min(h, ymin + mask_h)\n",
    "\n",
    "        # apply random color mask\n",
    "        image[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n",
    "\n",
    "        # return unobscured labels\n",
    "        if len(labels) and s > 0.03:\n",
    "            box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n",
    "            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n",
    "            labels = labels[ioa < 0.60]  # remove >60% obscured labels\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def create_folder(path='./new'):\n",
    "    # Create folder\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)  # delete output folder\n",
    "    os.makedirs(path)  # make new output folder\n",
    "\n",
    "\n",
    "def flatten_recursive(path='../coco128'):\n",
    "    # Flatten a recursive directory by bringing all files to top level\n",
    "    new_path = Path(path + '_flat')\n",
    "    create_folder(new_path)\n",
    "    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):\n",
    "        shutil.copyfile(file, new_path / Path(file).name)\n",
    "\n",
    "\n",
    "def extract_boxes(path='../coco128/'):  # from utils.datasets import *; extract_boxes('../coco128')\n",
    "    # Convert detection dataset into classification dataset, with one directory per class\n",
    "\n",
    "    path = Path(path)  # images dir\n",
    "    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # remove existing\n",
    "    files = list(path.rglob('*.*'))\n",
    "    n = len(files)  # number of files\n",
    "    for im_file in tqdm(files, total=n):\n",
    "        if im_file.suffix[1:] in img_formats:\n",
    "            # image\n",
    "            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB\n",
    "            h, w = im.shape[:2]\n",
    "\n",
    "            # labels\n",
    "            lb_file = Path(img2label_paths([str(im_file)])[0])\n",
    "            if Path(lb_file).exists():\n",
    "                with open(lb_file, 'r') as f:\n",
    "                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels\n",
    "\n",
    "                for j, x in enumerate(lb):\n",
    "                    c = int(x[0])  # class\n",
    "                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # new filename\n",
    "                    if not f.parent.is_dir():\n",
    "                        f.parent.mkdir(parents=True)\n",
    "\n",
    "                    b = x[1:] * [w, h, w, h]  # box\n",
    "                    # b[2:] = b[2:].max()  # rectangle to square\n",
    "                    b[2:] = b[2:] * 1.2 + 3  # pad\n",
    "                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n",
    "\n",
    "                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n",
    "                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n",
    "                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'\n",
    "\n",
    "\n",
    "def autosplit(path='../coco128', weights=(0.9, 0.1, 0.0), annotated_only=False):\n",
    "    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\n",
    "    Usage: from utils.datasets import *; autosplit('../coco128')\n",
    "    Arguments\n",
    "        path:           Path to images directory\n",
    "        weights:        Train, val, test weights (list)\n",
    "        annotated_only: Only use images with an annotated txt file\n",
    "    \"\"\"\n",
    "    path = Path(path)  # images dir\n",
    "    files = sum([list(path.rglob(f\"*.{img_ext}\")) for img_ext in img_formats], [])  # image files only\n",
    "    n = len(files)  # number of files\n",
    "    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\n",
    "\n",
    "    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\n",
    "    [(path / x).unlink() for x in txt if (path / x).exists()]  # remove existing\n",
    "\n",
    "    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\n",
    "    for i, img in tqdm(zip(indices, files), total=n):\n",
    "        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\n",
    "            with open(path / txt[i], 'a') as f:\n",
    "                f.write(str(img) + '\\n')  # add image to txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765b3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ObjDetect] *",
   "language": "python",
   "name": "conda-env-ObjDetect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
